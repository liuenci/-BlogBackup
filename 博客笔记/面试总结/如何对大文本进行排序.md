参考链接：https://blog.csdn.net/lemon_tree12138/article/details/48783535

前言：给定一个 5 亿大小的数据文本，要求你对它里面的数据进行排序。

第一感觉如果是用我们平常接触到的各种排序算法是不行的，因为这个数据量太大了，如果把这 5 亿数据都放到内存中去排序，内存可能会被撑爆。

我们有一个解决问题的思想叫做分治法，就是一步做不了的事情，分成两步去做，两步做不了就继续再分。

将存在文本中的数据分割成批量的小文件中。这里我们的做法是每次读取待排序文件的 10000 个数据，把这 10000 个数据进行快排，再写到一个小文件中，这样我们就得到了 50000 个已经排序好了的小文件了。在已排序小文件的基础之上，每次拿到文件中的最小值插入到最终的数据文本中。
